---
title: "Analysing long-term work and family dynamics with R package xyz"
author: "Santtu, Jouni, Satu & Simon & Sanni?"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Potential venues:

Sociological Methodology if go application first style, perhaps with another software paper on Journal of Open Source Software or similar "light" journal. I can see a proper stats paper coming from this as well. No need to decide yet.

## Introduction

Studying individual-level life history data is important and difficult. Multiple approaches exists to analyze such data, for example, sequence analysis, hidden Markov models, etc. Here we introduce a general modelling framework and accompanying R package for flexible joint modelling of the interdependent dynamics of multivariate measurements from multiple individuals where these dependency structures can vary in time. And we apply it to some real data and find something interesting.

Consider an individual $i$ with observations $y_{i,t} = (y^1_{i,t}, \ldots, y^c_{i,t},\ldots, y^C_{i,t})$, $t=1,\ldots,T$, i.e. at each time point $t$ we have $C$ observations from individual $i$. Assume that each element of $y_{i,t}$ can depend on the past values $y_{i,t-1}$, as well as additional exogenous covariates $x_{i,t}$. Then, assuming that the elements of $y_{i,t}$ are independent given $y_{i, t-1}$ and $x_{i,t}$, we have
$$
\begin{aligned}
y_{i,t} &\sim p_t(y_{i,t} | y_{i,t-1},  x_{i,t}) = p^1_t(y^1_{i,t} | y^1_{i,t-1}, y^2_{i,t-1}, x_{i,t})p^2_t(y^2_{i,t} | y^1_{i,t-1}, y^2_{i,t-1}, x_{i,t}).
\end{aligned}
$$
Here the parameters of the conditional distributions $p^1$ and $p^2$ depend on time, which allows us to take into account the fact that the dynamics of our system can evolve over time. For example, something something life courses.

For $p^c_t$, given a suitable link function depending our distributional assumptions, we define a linear predictor $\eta^c_{t}$ for the channel $c$ with a following general form:

$$
\begin{aligned}
\eta^c_{t} &= X^c_{t} \beta^c_{t}\\
\beta^c_{k,t} &= B_t \delta_k^c, \quad k=1,\ldots,K,
\end{aligned}
$$
where $K$ is the number of covariates, $B_t$ is a vector of B-spline values at time $t$ and $\delta_k^c$ is vector of corresponding spline coefficients. In general the number of B-splines $D$ (number of knots) used for constructing the splines for the study period $1,\ldots,T$ can be chosen freely, and can often result in overfitting. To mitigate this, we define a random walk prior for $\delta^c_k$ as

$$
\begin{aligned}
\delta^c_{k,1} &\sim N(0, 1),\\
\delta^c_{k,d} &\sim N(\delta^c_{k,d-1}, (\tau^c_k)^2), \quad d=2,\ldots, D.\\
\end{aligned}
$$

Here $\tau^c_k$ controls the "wigglyness" of the spline curves. In some cases it can be though that the splines change values in approximately same time points. For this, we can define (NOT YET TESTED) a global shrinkage series $\lambda_1,\ldots, \lambda T$ and define 
$$
\delta^c_{k,d} \sim N(\delta^c_{k,d-1}, (\tau^c_k\lambda_t)^2).
$$

# Example

(not run)
```{r, eval = FALSE}
library(rstan)
rstan_options(javascript = FALSE)

logsumexp <- function (x) {
  y = max(x)
  y + log(sum(exp(x - y)))
}

softmax <- function (x) {
  exp(x - logsumexp(x))
}

set.seed(8)
# number of time points
T <- 100 
# number of individuals
N <- 100
# number of observed symbols
S <- 4
# number of hidden states
M <- 10
# sample the hidden state sequence
states <- rep(1:M, times = rmultinom(1, T, rep(1, M)))

# exogenous predictor
x <- t(sin(1:(T+1)) + matrix(rnorm(N * (T + 1), sd = 0.2), T + 1, N))
# also explain with previous observations
# => number of predictors K = S + 1
# (intercept + x + factor(1:S))

K <- S + 1
# coefficients
beta <- array(0, c(S, K, M))
beta[-S, , 1] <- rnorm((S - 1) * K, sd = 0.5)

sigma <- 0.2 + abs(rnorm(K, sd = 0.1))
for(m in 2:M) {
  beta_change <- matrix(rnorm((S - 1) * K), S - 1, K)
  beta[, , m] <- beta[, , m - 1] + rbind(beta_change, 0) %*% diag(sigma)
}

ts.plot(t(matrix(beta, ncol = M)))


y <- matrix(NA, N, T) 
X <- array(0, c(N, T+1, K)) 
y0 <- factor(sample(1:S, N, TRUE), levels = 1:S)
X[, 1, ] <-  model.matrix( ~ x[, 1] + y0)
for(t in 1:T) {
  for(i in 1:N){
    y[i, t] <- sample(1:S, size = 1, prob = softmax(X[i, t, ] %*% t(beta[, , states[t]])))
  }
  X[, t + 1, ] <- model.matrix( ~ x[, t+1] + factor(y[, t], levels = 1:S))
}
# match indexing with stan
y <- t(y)
X <- aperm(X, c(2, 1, 3))
X <- X[-(T+1), , , drop = FALSE]

TraMineR::seqIplot(TraMineR::seqdef(t(y)))
TraMineR::seqdplot(TraMineR::seqdef(t(y)))

Xpred <- model.matrix( ~ x + y0, 
  data = data.frame(x = 0, y0 = factor(1:S, levels = 1:S)))

model <- stan_model("categorical_splines_best.stan")

library("splines")
# create the B-splines
B <- t(bs(1:T, knots = seq(1, T, by = 1), degree = 3, intercept = FALSE)) 
num_basis <- nrow(B)

d <- list(prior_only = 0, N = N, T = T, S = S, response = y, C = 1,
  X = X,  K = K, B = B, num_basis = num_basis,
  Xpred = Xpred)

fit <- sampling(model, data = d, 
  chains = 4, cores = 4,  
  refresh = 10, iter= 2000,
  seed = 1)
saveRDS(fit, file = "fit.rds")



ts.plot(cbind(
  apply(extract(fit, "beta",permuted=TRUE)[[1]], 2:4, mean)[,1,1],
  apply(extract(fit, "beta",permuted=TRUE)[[1]], 2:4, quantile,0.025)[,1,1],
  apply(extract(fit, "beta",permuted=TRUE)[[1]], 2:4, quantile,0.975)[,1,1]), col = 1)
points(rep(beta[1,1,], times = rle(states)$lengths))

pred <- extract(fit, "pred_response")[[1]]
predmeans <- apply(pred, 2:4, mean)
k <- 1
ts.plot(cbind(
  predmeans[, 1:S, k],
  t(sapply(1:M, function(i) softmax(Xpred[k, ] %*% t(beta[, , i]))))[states,]), 
  col = 1:S, lty = rep(1:2, each = S))

```

Stan code:

```{stan, eval = FALSE, output.var = "model"}
data {
  
  int<lower=1> T; // number of time points
  int<lower=1> N; // number of individuals
  int<lower=1> C; // number of channels/response variables
  int response[T,N];
  int<lower=0> K; //number of covariates
  matrix[N, K] X[T]; //covariates
  int<lower=0> S; // number of observed symbols
  
  matrix[S, K] Xpred; // for predictions
  
  int num_basis;
  matrix[num_basis, T] B; // for splines
  
}
transformed data {
  row_vector[K] zeros_K = rep_row_vector(0, K);
  vector[S] zeros_S = rep_vector(0, S);
}

parameters {
  row_vector[num_basis] a_raw[S-1, K]; // S - 1 as we need to fix one set of betas to zero
  vector<lower=0>[K] tau; 
}

transformed parameters {
  matrix[S, K] beta[T];
  row_vector[num_basis] a[S-1, K]; 
  // strangely this seems to be faster than working with array of matrices etc...
  for(s in 1:(S-1)) {
    for(k in 1:K) {
      a[s, k, 1] = a_raw[s, k, 1];
      for (i in 2:num_basis) {
        a[s, k, i] = a[s, k, i-1] + a_raw[s, k, i] * tau[k];
      }
      for(t in 1:T){
        beta[t, s, k] = a[s, k] * B[, t];
      } 
    }
  }
  for(t in 1:T){
    beta[t, S, ] = zeros_K;
  } 
}

model {
  
  tau ~ std_normal();
  for(s in 1:(S-1)) {
    for(k in 1:K) {
      a_raw[s, k] ~ std_normal();
    }
  }
  for(t in 1:T) {
    target += categorical_logit_glm_lupmf(response[t] | X[t], zeros_S, beta[t]');
  }
}

generated quantities {
  matrix[S, S] pred_response[T];
  for(t in 1:T) {
    for(s in 1:S) {
      pred_response[t, , s] = softmax(beta[t] * Xpred[s, ]');
    }
  }
}

```

# Main functionality of the package

- How to build and estimate the model, supported distributions, how to define priors...
- How to interpret the results, visualization, posterior predictive checks, causal queries based on the posterior predictive distribution?

# Application

# Results

# Discussion


