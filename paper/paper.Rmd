---
title: ""
author: ""
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Studying individual-level life history data is important and difficult... Multiple approaches exists, sequence analysis, hidden Markov models, etc. We introduce an R package for joint modelling of the interdependent dynamics of multivariate measurements from multiple individuals where these dependency structures can vary in time.

Consider an individual with observations $y_t = (y^1_t, y^2_t)$, $t=1,\ldots,T$, i.e. at each time we have two observations $y_t$ and $y'_t$. We assume a double chain dynamics (also known as Markov switching model, regime switching model)

$$
\begin{aligned}
y_t &= p_{z_t}(y_t | y_{t-1},  x_t) = p^1_{z_t}(y^1_t | y^1_{t-1}, y^2_{t-1}, x_t)p^2_{z_t}(y^2_t | y^1_{t-1}, y^2_{t-1}, x_t),\\
z_t &= p(z_t | z_{t-1}),
\end{aligned}
$$
where $x_t$ contains exogenous covariates and $z_t$ is Markov process with finite state space so that 
$P(z_t = i | z_{t-1} = j) = A_{i,j}$, for $i,j=1,\ldots,M$. Compared to typical hidden Markov model where $p_{z_t}(y_t | y_{t-1},  x_t) = p_{z_t}(y_t |  x_t)$, this allows more complex and often more realistic serial dependency, and allows natural way to study causal relationships between the observations (assuming that the model correctly describes the underlying data generating process).

As a DAG, we have (with variables $X$ omitted for simplicity)

![Double chain model](dcm.png)

We assume that $p_{z_t}(y_t | y_{t-1}, x_t)$ has identical parametric form independent of the state variable $z_t$, but that the parameters $\theta$ of that distribution vary according to the value of $z_t$. For example, when $y_t$ is univariate Gaussian, we might have $y_t \sim N(\phi^{(z_t)} y_{t-1},{\sigma^2}^{(z_t)})$ with $\theta^{(z_t)} = (\phi^{(z_t)},\sigma^{(z_t)})$.

In a typical setting where we have multiple subjects we assume that they are governed by the same latent state process $z_1,\ldots,z_T$ i.e. we assume that the regimes correspond to global, population-level behaviour. We make this assumption for several reasons: First, this model is often more easily interpretable compared to a case with individuals latent processes for each individual. Second, with this assumption the model scales better with respect to the number of individuals, making it computationally more feasible for large data. Finally, hidden Markov models and other mixture models often exhibit strong multimodality, which leads to both computational and inferential difficulties. The use of single common latent process can alleviate these problems.

Similarly, partially due to identifiability and partially due to interpretability purposes, we assume that the transition matrix $A$ has a strict left-to-right form so that $A_{i,j}=0$ if $j < i$ or $j>i+1$. This leads to a change-point model with fixed number of change points, happening at unknown times. Finally, we use random walk prior structure for the penalize the variability in $\theta$, i.e. we assume 
$$
\theta^{(m)} \sim N(\theta^{(m-1)}, \Sigma_\theta), \quad m=2,\ldots,M
$$
with some prior for $\theta^{(1)}$. Furthermore, as we know the start and end states (with the reasonable assumption that the last state should be reached by time $T$ in order to it to be nonredundant), we can reduce potential multimodality issues by fixing $z_t = 1$ for $t=1,\ldots,\tau_{start}$ and $z_t = M$ for $t=T-\tau_{end},\ldots,T$, with $\tau_{start} \geq 1$ and $\tau_{end} \leq T$ chosen based on the application. Note that when $M \to T$, $A_{i,i} \to 0$ this model becomes time-varying random walk coefficient model. However, we typically assume that $M << T$ i.e. instead of constantly evolving parameters there are only small number of change points.

## Simulation experiment

We consider a simple case where all observations are linear-Gaussian with

$$
p(y^j_t | y_{t-1}, z_t) = N(\alpha^{(z_t)}_j + \sum_{i=1}^C \beta^{(z_t)}_{ij}y^i_{t-1}, {\sigma^2}^{(z_t)}), \qquad  t=1,\ldots,T,\quad j=1,\ldots,C,  \quad z_t \in \{1,\ldots,M\}.
$$


We use `rstan` development version 2.26.1 which is somewhat faster than the one in CRAN, and which also supports some additional functions important to us (the Gaussian version should actually work with CRAN version as well). This version can be installed as
```{r, eval = FALSE}
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

We assume 3 hidden states, 20 time points, 100 sequences and 2 channels, with
$$
z_t = \begin{cases} 
1 & t\leq 5 \\
2 & 5\lt t \leq 13 \\
3 & 13 \lt t \leq 20
\end{cases}
$$
First we simulate the data
```{r}
set.seed(1)
M <- 3
C <- 2
T <- 20
N <- 100
z <- rep(1:M, times = c(5, 8, 7))


# intercept terms
alpha <- matrix(NA, C, M)
alpha[, 1] <- rnorm(C)
for(m in 2:M) alpha[, m] <- rnorm(C, alpha[, m - 1], 0.1)

# regression coefficients
# column c gives coefficients for response channel c
beta <- array(NA, c(C, C, M))
# Assume |beta^1| < 0.5 so series do not explode so easily
beta[,, 1] <- runif(C^2, -0.5, 0.5)
for(m in 2:M) beta[, , m] <- rnorm(C^2, beta[, , m - 1], 0.1)

# standard deviations, random walk in log-scale
sigma <- matrix(NA, C, M)
sigma[, 1] <- rnorm(C)
for(m in 2:M) sigma[, m] <- rnorm(C, sigma[, m - 1], 0.1)
sigma <- exp(sigma)

# generate observations
y <- array(0, c(T, N, C))
for(i in 1:N) {
  # first time point from normal distribution assuming y_0 = 0
  y[1, i, ] <- rnorm(C, alpha[, 1], sigma[, 1])
  for(t in 2:T) {
    y[t, i, ] <- rnorm(C, alpha[, z[t]] + y[t - 1, i, ] %*% beta[, , z[t]], sigma[, z[t]])
  }
}

d <- list(y = y, M = M, N = N, C = C, T = T, T_fixed_start = 1, T_fixed_end = 1)
```

use `because` (change the name??):